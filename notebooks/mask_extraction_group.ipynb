{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7204113",
   "metadata": {},
   "source": [
    "HP event segmentation project: \n",
    "re-preprocessing fMRI data and mapping into shared response space\n",
    "\n",
    "1) The preprocessed Harry Potter fMRI dataset fMRI data download from: https://drive.google.com/file/d/1aYEZZSyrlo0UqswDBUiGzE3kGl3RcCn8/view?usp=sharing\n",
    "2) The fMRI mask is a processed mask from https://github.com/anikethjr/brain_syntactic_representations/blob/main/mask_sub_fedorenko_HP_narrow_mirrored.npy\n",
    "3) Other infomation about the data can be found at https://github.com/anikethjr/brain_syntactic_representations\n",
    "\n",
    "25 Sep 2023 by Huidong Xue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f4b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy import io as sio\n",
    "import scipy.spatial.distance as sp_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "from brainiak.isc import isc\n",
    "from brainiak.fcma.util import compute_correlation\n",
    "import brainiak.funcalign.srm\n",
    "from brainiak import image, io\n",
    "from brainiak.isc import isc\n",
    "import brainiak.funcalign.srm\n",
    "from brainiak import image, io\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bffd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded F.npy\n",
      "Downloaded H.npy\n",
      "Downloaded I.npy\n",
      "Downloaded J.npy\n",
      "Downloaded K.npy\n",
      "Downloaded L.npy\n",
      "Downloaded M.npy\n",
      "Downloaded N.npy\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "base_url = \"https://github.com/anikethjr/brain_syntactic_representations/raw/main/masks/\"\n",
    "subjects = [\"F.npy\", \"H.npy\", \"I.npy\", \"J.npy\", \"K.npy\", \"L.npy\", \"M.npy\", \"N.npy\"]\n",
    "dest_dir = \"/Volumes/my drive/thesis_pipeline/data/masks\"\n",
    "\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "for subject in subjects:\n",
    "    url = base_url + subject\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(dest_dir, subject), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {subject}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {subject}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ffa2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/my drive/thesis_pipeline\"\n",
    "masks_path = os.path.join(base_path, \"data\", \"masks\")\n",
    "sub_space_data_path = os.path.join(base_path, \"data\", \"sub_space_data\")\n",
    "output_path = os.path.join(base_path, \"sub_space_ROIdata\")\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586c88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects = [\"F\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\"]\n",
    "roi_names = [\n",
    "    'PTL',\n",
    "    'ATL',\n",
    "    'AG',\n",
    "    'IFG',\n",
    "    'MFG',\n",
    "    'IFGorb'\n",
    "]\n",
    "n_rois = len(roi_names)\n",
    "NS = len(all_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7150df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sub_path = os.path.join(masks_path, \"mask_sub_fedorenko_HP_narrow_mirrored.npy\")\n",
    "mask_sub = np.load(mask_sub_path, allow_pickle=True, fix_imports=True)[()]\n",
    "\n",
    "\n",
    "mask_num = dict([(s, mask_sub[s][np.load(os.path.join(masks_path, \"{}.npy\".format(s)))])\n",
    "                 for s in all_subjects])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1faaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "(1291, 27905)\n",
      "H\n",
      "(1291, 24983)\n",
      "I\n",
      "(1291, 25263)\n",
      "J\n",
      "(1291, 29650)\n",
      "K\n",
      "(1291, 25003)\n",
      "L\n",
      "(1291, 24678)\n",
      "M\n",
      "(1291, 28752)\n",
      "N\n",
      "(1291, 24397)\n"
     ]
    }
   ],
   "source": [
    "ROI_sub_data = []\n",
    "\n",
    "for i_s, sub in enumerate(all_subjects):\n",
    "    mask = np.load(os.path.join(masks_path, f\"{sub}.npy\"))\n",
    "    data = np.load(os.path.join(sub_space_data_path, f\"{sub}.npy\"))\n",
    "    print(sub)\n",
    "    print(data.shape)\n",
    "    for i, roi in enumerate(roi_names): \n",
    "        roi_mask = np.where(mask_num[sub]==(i+1))[0]\n",
    "        roi_data = data[:, roi_mask]\n",
    "        \n",
    "        data_point = {}\n",
    "        data_point[\"Subject\"] = sub\n",
    "        data_point[\"ROI\"] = roi\n",
    "        data_point[\"data\"] = roi_data\n",
    "        \n",
    "        ROI_sub_data.append(data_point)\n",
    "\n",
    "ROI_sub_data = pd.DataFrame(ROI_sub_data)\n",
    "\n",
    "np.save(os.path.join(output_path, \"HP_ROI_data.npy\"), ROI_sub_data)\n",
    "sio.savemat(os.path.join(output_path, \"HP_ROI_data.mat\"), {Subject: col.values for Subject, col in ROI_sub_data.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf728e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "srm_data_path = os.path.join(base_path, \"SRM_data\")\n",
    "if not os.path.exists(srm_data_path):\n",
    "    os.mkdir(srm_data_path)\n",
    "\n",
    "# Load the ROI data\n",
    "data = np.load(os.path.join(output_path, \"HP_ROI_data.npy\"), allow_pickle=True)\n",
    "\n",
    "# Prepare the dataset for SRM, create list for each ROIs\n",
    "for ROIi in roi_names:\n",
    "    subroidata = data[np.where(data[:, 1] == ROIi)]\n",
    "    roidata = subroidata[:, 2]\n",
    "    \n",
    "    # Normalize the data for each run\n",
    "    run1 = [stats.zscore(d[0:325, :], axis=0) for d in roidata[:]]\n",
    "    run2 = [stats.zscore(d[325:662, :], axis=0) for d in roidata[:]]\n",
    "    run3 = [stats.zscore(d[662:926, :], axis=0) for d in roidata[:]]\n",
    "    run4 = [stats.zscore(d[926:1291, :], axis=0) for d in roidata[:]]\n",
    "    \n",
    "    roidata_Z = [np.concatenate([sublist1, sublist2, sublist3, sublist4], axis=0) for sublist1, sublist2, sublist3, sublist4 in zip(run1, run2, run3, run4)]\n",
    "    \n",
    "    # Save the normalized data\n",
    "    with open(os.path.join(srm_data_path, f\"{ROIi}_dataZ.pkl\"), 'wb') as file:\n",
    "        pickle.dump(roidata_Z, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f641e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "srm_data_norTS_path = os.path.join(base_path, \"SRM_data_norTS\")\n",
    "num_subs = len(all_subjects)\n",
    "\n",
    "# Create the SRM normalized data directory if it doesn't exist\n",
    "if not os.path.exists(srm_data_norTS_path):\n",
    "    os.makedirs(srm_data_norTS_path)\n",
    "\n",
    "# Load the data and normalize the data across runs (the data was normalized for each run)\n",
    "nVox_ROI = []\n",
    "\n",
    "for ROIi in roi_names:\n",
    "    with open(os.path.join(srm_data_path, f\"{ROIi}_dataZ.pkl\"), 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        \n",
    "        # Iterate through the subjects\n",
    "        for sub in range(num_subs): \n",
    "            # Find rows where all elements are not NaN    \n",
    "            non_nan_rows = ~np.any(np.isnan(data[sub]), axis=0)\n",
    "            data[sub] = data[sub][:, non_nan_rows]    \n",
    "            \n",
    "            # Normalize the data\n",
    "            data[sub] = stats.zscore(data[sub], axis=1, ddof=1)\n",
    "    \n",
    "    with open(os.path.join(srm_data_norTS_path, f\"{ROIi}_dataTVZ_NANnorTS.pkl\"), 'wb') as datafile:\n",
    "        pickle.dump(data, datafile)\n",
    "            \n",
    "    nVox = [len(data[i][0]) for i in range(num_subs)]\n",
    "    nVox_ROI.append(nVox)\n",
    "\n",
    "with open(os.path.join(srm_data_path, f\"{ROIi}_nGoodVox.pkl\"), 'wb') as nVoxfile:\n",
    "    pickle.dump(nVox_ROI, nVoxfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "840895be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SRM, may take a minute ...\n",
      "SRM has been fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danbivol/miniconda3/envs/potter_analysis/lib/python3.7/site-packages/numpy/lib/npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SRM, may take a minute ...\n",
      "SRM has been fit\n",
      "Fitting SRM, may take a minute ...\n",
      "SRM has been fit\n",
      "Fitting SRM, may take a minute ...\n",
      "SRM has been fit\n",
      "Fitting SRM, may take a minute ...\n",
      "SRM has been fit\n",
      "Fitting SRM, may take a minute ...\n",
      "SRM has been fit\n"
     ]
    }
   ],
   "source": [
    "srm_data_norTS_path = os.path.join(base_path, \"SRM_data_norTS\")\n",
    "\n",
    "\n",
    "for ROIj in roi_names:\n",
    "    with open(os.path.join(srm_data_norTS_path, f\"{ROIj}_dataTVZ_NANnorTS.pkl\"), 'rb') as datafSRMfile:\n",
    "        datafSRMraw = pickle.load(datafSRMfile)\n",
    "        # The model requires a list of 2D arrays, element i has shape=[voxels_i, samples]\n",
    "        # Each element in the list contains the fMRI data of one subject.\n",
    "        datafSRM = [d.T for d in datafSRMraw[:]]\n",
    "    \n",
    "    # Estimating the SRM\n",
    "    features = 90  # Number of features \n",
    "    n_iter = 10  # Number of iterations of fitting\n",
    "    \n",
    "    # Create the SRM object\n",
    "    srm = brainiak.funcalign.srm.SRM(n_iter=n_iter, features=features)\n",
    "    \n",
    "    # Fit the SRM data\n",
    "    print('Fitting SRM, may take a minute ...')\n",
    "    srm.fit(datafSRM)\n",
    "    \n",
    "    print('SRM has been fit')\n",
    "        \n",
    "    output_file_path = os.path.join(srm_data_path, f\"{ROIj}_srm\")\n",
    "    srm.save(output_file_path)\n",
    "    \n",
    "    # Use the model to transform matrix to Shared Response space\n",
    "    shared = srm.transform(datafSRM)\n",
    "    with open(os.path.join(srm_data_path, f\"{ROIj}_shareddata.pkl\"), 'wb') as sharedfile:\n",
    "        pickle.dump(shared, sharedfile)\n",
    "    \n",
    "    with open(os.path.join(srm_data_norTS_path, f\"{ROIj}_dataTVZ_NANnorTS_SRM.pkl\"), 'wb') as useddatafile:\n",
    "        pickle.dump(datafSRM, useddatafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5ac53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
